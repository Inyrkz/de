# Basic Curriculum

![](https://user-images.githubusercontent.com/62965911/215325722-1f79be37-9ec0-4fd1-ab0f-b424b0365853.svg)

##### Table of Contents

- [Development Foundations](#development-foundations)
- [Data Engineering Foundations](#data-engineering-foundations)
- [Cloud Computing and Data Engineering tools in AWS](#cloud-computing-and-data-engineering-tools-in-aws)
- [Advanced Python Programming](#advanced-python-programming)
- [SQL Programming](#sql-programming)
- [Advanced PySpark Programming](#advanced-pyspark-programming)
- [Scala Programming](#scala-programming)
- [Data Modeling with RDBMS/NoSQL](#data-modeling-with-rdbms-nosql)
- [Data Warehouse and Data Lakes](#data-warehouse-and-data-lakes)
- [Big Data Processing](#big-data-processing)
- [Building Data Pipelines](#building-data-pipelines)
- [End-to-End Industry Grade Project](#end-to-end-industry-grade-project)

##### Estimated Time:
   - Monologue mode: 20 hrs
   - Interactive mode: 35 hours

## Development Foundations

### Tools and Concepts

1. Visual Studio Code (VSCode) 
2. Anaconda
3. Github
4. Bash Shell

### Labs

1. VSCode GUI and Features
2. Github basics
3. Bash Shell commands
4. Setting up an integrated workspace in VSCode

## Data Engineering Foundations

### Tools and Concepts

1. What is Data Engineering?
2. Role of Data Engineering in Organizations
3. Skills required to become a Data Engineer

## Cloud Computing and Data Engineering tools in AWS

### Tools and Concepts

1. Benefits of cloud computing
2. Types of cloud computing
3. AWS S3
4. RDS
5. Redshift
6. AWS Glue
7. Athena
8. DBeaver

### Labs

1. Create AWS Cloud Account
2. Working with AWS IAM Service
3. Create AWS IAM Credentials
4. Setup AWS CLI in VSCode
1. Copy and Sync data to/from S3
2. Create Postgres/MySQL DBMS in RDS
3. Create Redshift Warehouse
6. Install DBeaver in your PC
4. Connect to Postgres Database
5. Connect to Redshift Warehouse
6. Create AWS Glue Crawlers and Databases
7. Run SQL Queries in Athena

## Advanced Python Programming

### Tools and Concepts

1. Python Functions and Classes
2. Pandas Series and Dataframes

### Labs

1. Building Functions and Classes in Python
1. Read/Write and Manipulate Data using Pandas
2. Data Format Conversion - CSV to Parquet, JSON to CSV/Parquet
3. Pulling Data from APIs using requests library
4. Connect to Postgres and Redshift from Python
5. Load and Read the data from Postgres and Redshift using Python

## SQL Programming

### Labs

1. Hospital Data Analysis with SQL
2. Ecommerce Data Analysis with SQL
3. Order Analysis with Redshift SQL

## Advanced PySpark Programming

### Tools and Concepts

1. Spark and Hadoop Fundamentals
2. Databricks
3. Spark UDFs
4. Spark Dataframe API

### Labs

1. Create Databricks Account
2. Create Spark Cluster in Databricks
3. Data Transformation with PySpark
4. Connect AWS to PySpark
5. ETL Pipeline with AWS S3 and PySpark

## Scala Programming

### Labs

1. Introduction to Scala Programming
2. Transform complex data types

## Data Modeling with RDBMS/NoSQL 

### Tools and Concepts

1. Data Modeling
2. SQL vs NoSQL
3. Star and Snowflake Schema
4. Postgres
5. Cassandra

### Labs

1. Music Data Modeling with Postgres
2. Music Data Modeling with Cassandra
3. Healthcare Data Modeling with Postgres

## Data Warehouse and Data Lakes 

### Tools and Concepts

1. Data Warehouses vs Data Lakes
2. Data Lakes and Lakehouses

### Labs

1. Loading Data into Redshift
2. Queries in Redshift
3. Data lake with AWS, S3 and Athena
4. Working with Delta lake in Databricks

## Big Data Processing

### Tools and Concepts

1. Spark Jobs
2. EMR and EMR Serverless
3. Databricks
4. dbt

### Labs

1. Processing data with EMR Serverless
2. Processing data using dbt in Redshift
3. Processing data with Databricks

## Building Data Pipelines

### Tools and Concepts

1. Data Pipelines (ETL/ELT)
1. Apache Airflow
   1. UI
   2. Operators
   3. Variables
   4. Plugins
   5. Schedules
   6. etc.

### Labs

1. Install Airflow in your PC
2. Install Airflow in cloud
3. Building an ETL Data Pipeline with python and aws data lake
4. Building an ETL Data Pipeline with lambda and aws data lake
5. Building an ELT Data Pipeline with dbt, airflow and redshift

## End-to-End Industry Grade Project 

- We will build an end-to-end project with tools and concepts we learned and adopted in the trainin sessions.
- The project will be added in your resume.
- We will also release the project in your github. (Recruiters are interested in seeing your projects in git)