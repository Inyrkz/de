# Basic Curriculum

![](https://user-images.githubusercontent.com/62965911/215325722-1f79be37-9ec0-4fd1-ab0f-b424b0365853.svg)

##### Table of Contents

- [Developer Foundations](#developer-foundations)
- [Data Engineering Foundations](#data-engineering-foundations)
- [Cloud Computing and Data Engineering tools in AWS](#cloud-computing-and-data-engineering-tools-in-aws)
- [Python Programming](#python-programming)
- [SQL Programming](#sql-programming)
- [PySpark Programming](#pyspark-programming)
- [Scala Programming](#scala-programming)
- [Data Modeling with RDBMS/NoSQL](#data-modeling-with-rdbms-nosql)
- [Data Warehouse and Data Lakes](#data-warehouse-and-data-lakes)
- [Big Data Processing](#big-data-processing)
- [Building Data Pipelines](#building-data-pipelines)
- [End-to-End Industry Grade Project](#end-to-end-industry-grade-project)

##### Estimated Time:

- Monologue mode: 20 hrs
- Interactive mode: 35 hours

## Developer Foundations

Tools: Visual Studio Code (VSCode), Anaconda Python, Github and Bash Shell

**Labs**

* [ ] Download and Install vscode
* [ ] Understand vscode features
* [ ] Install extensions in vscode
* [ ] Download and Install Anaconda
* [ ] Create virtual environment in anaconda
* [ ] Create jupyter notebook in vscode and connect to venv
* [ ] Create github account
* [ ] Install git cli
* [ ] Create git repo and add students as collaborator
* [ ] Connect local workspace to git repo
* [ ] Learn git commands
* [ ] Learn bash commands
* [ ] Download and Install DBeaver

## Data Engineering Foundations

In this module, we will learn the basic concepts we should know as a data engineer. We will focus primarily on the following set of questions:

1. What is Data Engineering?
2. Role of Data Engineering in Organizations
3. Skills required to become a Data Engineer
4. What is data lake and data warehouse?
5. What is medallion architecture?
6. What is EL, ETL and ELT?
7. What are the benefits of cloud computing?
8. OLTP vs OLAP technologies

## Cloud Computing and Data Engineering tools in AWS

Tools: AWS S3, RDS, Redshift, AWS Glue, Athena, AWS Lambda, EMR and EMR Serverless, Keyspace, Cloudformation, AWS IAM, Secrets Manager

**Labs**

1. [ ] (Optional) Create AWS Account
2. [ ] Create IAM user and generate credentials
3. [ ] Install AWS CLI
4. [ ] Setup AWS credentials
5. [ ] Walkthrough of various AWS Services
6. [ ] Copy and Sync data to/from S3
7. [ ] Create database in RDS DBMS and generate credentials
8. [ ] Connect to RDS DBMS in DBeaver
9. [ ] Pull credentials from Secrets Manager in Python using Boto3

## Python Programming

In this module, we will learn the essential python concepts we use in data engineering. We will primarily focus on the following topics:

1. Lists and dictionaries
2. For loops and while loops
3. Functions and Inline functions
4. Pandas Dataframes
5. `requests` library
6. `psycopg2` and `sqlalchemy` library

**Labs**

1. Building Functions in Python
2. Read/Write and Manipulate Data using Pandas
3. Data Format Conversion - CSV to Parquet, JSON to CSV/Parquet
4. Pulling Data from APIs using requests library
5. Connect to Postgres and Redshift from Python
6. Load and Read the data from Postgres using Python

## SQL Programming

### Labs

1. [ ] SQL Basic - SELECT, LIMIT, WHERE, Comparison and Logical Operators, ORDER BY
2. [ ] SQL Intermediate - Aggregation Functions, GROUP BY, CASE, JOINS
3. [ ] SQL Advanced - Dates, Texts, Subqueries, Window Functions, EXPLAIN

## PySpark Programming

### Tools and Concepts

1. Spark and Hadoop Fundamentals
2. Databricks
3. Spark UDFs
4. Spark Dataframe API

### Labs

1. [ ] Create Databricks Account
2. [ ] Create Spark Cluster in Databricks
3. [ ] M&M Analysis
4. [ ] Movielens and Song Analysis
5. [ ] San Francisco Fire Department Analysis
6. [ ] Data Transformation with PySpark
7. [ ] Connect AWS to PySpark
8. [ ] ETL Pipeline with AWS S3 and PySpark

## Scala Programming

### Labs

1. [ ] Introduction to Scala Programming
2. [ ] Transform complex data types
3. [ ] Extract and Load Process with Spark Scala, S3 and Postgres

## Data Modeling with RDBMS/NoSQL

### Tools and Concepts

1. Data Modeling
2. SQL vs NoSQL
3. Star and Snowflake Schema
4. Postgres
5. Cassandra

### Labs

1. [ ] Music Data Modeling with Postgres
2. [ ] Music Data Modeling with Cassandra
3. [ ] Healthcare Data Modeling with Postgres

## Data Warehouse and Data Lakes

### Tools and Concepts

1. Data Warehouses vs Data Lakes
2. Data Lakes and Lakehouses

### Labs

1. [ ] Loading Data into Redshift
2. [ ] Queries in Redshift
3. [ ] Data lake with AWS, S3 and Athena
4. [ ] Working with Delta lake in Databricks

## Big Data Processing

### Tools and Concepts

1. Spark Jobs
2. EMR and EMR Serverless
3. Databricks
4. dbt

### Labs

1. [ ] Processing data with EMR Serverless
2. [ ] Processing data using dbt in Redshift
3. [ ] Processing data with Databricks

## Building Data Pipelines

### Tools and Concepts

1. Data Pipelines (ETL/ELT)
2. Apache Airflow
   1. UI
   2. Operators
   3. Variables
   4. Plugins
   5. Schedules
   6. etc.

### Labs

1. [ ] Install Airflow in your PC
2. [ ] First DAG of executing Bash commands in Pipeline
3. [ ] CSV to JSON ETL Pipeline
4. [ ] ACLED Data Pipeline

## End-to-End Industry Grade Project

- We will build an end-to-end project with tools and concepts we learned and adopted in the training sessions.
- The project will be added in your resume.
- We will also release the project in your github. (Recruiters are interested in seeing your projects in git)
